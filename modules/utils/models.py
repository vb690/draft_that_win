import os

import json
from joblib import dump, load

from sklearn.base import clone

from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.layers import LSTM, Embedding
from tensorflow.keras.layers import Dropout, Activation

from tensorflow.keras.models import model_from_json

from kerastuner import HyperModel
from kerastuner.tuners import Sklearn as sk_tuner


class AbstractHyperNeuralNet(HyperModel):
    """ Abstract class implementing methods inherithed by an Hypermodel. An
    Hypermodel is a keras model written to be optimized with kerastuner.
    """
    MODELS = {}

    def __clone_best_model(self):
        """ Private method for cloning the best model, not "real" cloning
        happens but rather loading the acritecture without its associated
        weights.
        """
        path = f'{self.clone_dir}\\{self.model_tag}_best'
        print(path)
        with open(f'{path}\\architecture.json') as inp:
            fitted_model = json.load(inp)
        fitted_model = model_from_json(fitted_model)
        fitted_model.compile(
            loss=self.best_model.loss,
            optimizer=self.best_model.optimizer,
            metrics=self.best_model.metrics
        )
        setattr(
            self,
            'fitted_model',
            fitted_model
        )

    def _fc_block(self, hp, input_tensor, bn, do, max_depth=2):
        """ Protected method genearting a block of fuly connected layers (FCB)
        Each block can have an arbitrary number of layer each one having an
        arbitrary number of units
        Batch normalization and dropout are applied between each layer (if
        the tuner allows it)
        Args:
            - hp:               kerastuner hyperparameter object, object passed
                                to the Oracle for keeping track of the hp space
            - input_tensor:     keras tensor, previous portion of the comput
                                tational graph
            - bn:               bolean, determines if BatchNormalization will
                                be used
            - do:               Float, specifies the model dropout rate.
            - max_depth:        Integer specifying the maximum number of layers
                                that the tuner can propose.
        Returns:
            - fully_connected: a keras tesnor, portion of graph generated
                               by FCB
        """
        layers = hp.Int(
            min_value=1,
            max_value=max_depth,
            name='number_of_layers'
        )
        previous_layer = input_tensor
        for layer in range(layers):

            new_layer = Dense(
                units=hp.Int(
                    min_value=32,
                    max_value=224,
                    step=32,
                    name='dense_units_{}'.format(layer)
                )
            )(previous_layer)
            previous_layer = new_layer

        chosen_activation = hp.Choice(
                        values=['elu', 'relu', 'sigmoid', 'tanh'],
                        name='dense_activation_{}'.format(layer),
                    )
        previous_layer = Activation(
            chosen_activation,
            )(previous_layer)

        if bn:
            previous_layer = BatchNormalization()(previous_layer)

        previous_layer = Dropout(do)(previous_layer)

        return previous_layer

    def _emb_block(self, hp, input_tensor, bn, do):
        """ Protected method genearting an embedding block (EB)
        Batch normalization and dropout are applied after the embedding layer
        (if the tuner allows it).
        Args:
            - hp:               Kerastuner hyperparameter object, object passed
                                to the Oracle for keeping track of the hp space
            - input_tensor:     Keras tensor, previous portion of the comput
                                tational graph
            - bn:               Bolean, determines if BatchNormalization will
                                be used
            - do:               Float, specifies the model dropout rate.

        Returns:
            - embedding: a keras tesnor, portion of computational graph
                         generated by EB
        """
        embedding = Embedding(
            input_dim=300,
            output_dim=hp.Int(
                min_value=25,
                max_value=250,
                step=25,
                name='emb_size'
            )
        )(input_tensor)
        if bn:
            embedding = BatchNormalization()(embedding)
        embedding = Dropout(do)(embedding)

        return embedding

    def _rnn_block(self, hp, input_tensor, bn, max_depth=2):
        """ Protected method genearting a block of Recurrent layers (RB)
        Each block can have an arbitrary number of layer each one having an
        arbitrary number of units
        Batch normalization is applied at the end (if the tuner
        allows it)
        Args:
            - hp:               Kerastuner hyperparameter object, object passed
                                to the Oracle for keeping track of the hp space
            - input_tensor:     Keras tensor, previous portion of the comput
                                tational graph
            - bn:               Bolean, determines if BatchNormalization will
                                be used
            - max_depth:        Integer specifying the maximum number of layers
                                that the tuner can propose.

        Returns:
            - fully_connected: a keras tesnor, portion of graph generated
                               by FCB
        """
        layers = hp.Int(
            min_value=1,
            max_value=max_depth,
            name='number_of_layers_rnn'
        )
        previous_layer = input_tensor
        for layer in range(layers):

            new_layer = LSTM(
                units=hp.Int(
                    min_value=32,
                    max_value=224,
                    step=32,
                    name='lstm_units_{}'.format(layer)
                ),
                return_sequences=True
            )(previous_layer)
            previous_layer = new_layer

        if bn:
            rnn = BatchNormalization()(previous_layer)
        else:
            rnn = previous_layer

        return rnn

    def tune(self, X, y, tuner, epochs=30, callbacks=None,
             verbose=2, val_split=0.2, num_models=5, **kwargs):
        """Method used for tuning the hypermodel which inherit from the
        abstract class.
        No callback is passed but it is HIGHLY advisable to pass an
        EarlyStopping callback.
        Args:
            - X: numpy array, input to the hypermodel
            - y: numpy array, target of the hypermodel
            - tuner: a kerastuner tuner, one of random search, hyperband
                     or gaussian process
            - epochs: integer, maximum number of interations that a specific
                      configuration proposed by the oracle will be run for
            - callbacks: keras callback or callable, callbacks passed to the
                         hypermodel during tuning.
            - verbose: integer, level of verbosity for the hypermodel
            - num_models: integer, number of best models to keep after the
                          tuning
            - **kwargs: keyword aruments that need to be passed to the tuner
        Returns:
            - models: an iterable of keras models, list of the best
                      num_models already compiled and trained on the given X
                      and Y
        """
        if callbacks is None:
            print('No callback was passed, not using an EarlyStopping \
            callback might lead to poor model selection.')

        tuner_obj = tuner(
            hypermodel=self.build,
            **kwargs
        )
        tuner_obj.search(
            X,
            y,
            epochs=epochs,
            validation_split=val_split,
            callbacks=callbacks,
            verbose=verbose
        )
        # print the result for each configuration
        tuner_obj.results_summary()
        # we keep only the best model
        best_models = tuner_obj.get_best_models(num_models=num_models)
        setattr(self, 'best_model', best_models[0])
        # this dictionary structure allow to retrieve the models in
        # order (from best to worst) and for tags, it comes in handy with
        # sklearn where we have a collection of different model types
        for index, model in enumerate(best_models):

            self.MODELS[index] = {self.model_tag: model}

        return self.MODELS

    def save_m(self, save_n=1, save_dir='results\\saved_models'):
        """Method for saving the best n_models, we create a directory for
        each model, each directory will contain an architecture.json and
        weights.h5 for assembling the model.
        Args:
            - save_n: integer specifying the number of models to be saved
            - save_dir: string, directory where to save the models, the
                        default value assumes that this method is called
                        level of the hierarchy
        Returns:
            - None
        """
        len_models = len(self.MODELS)
        if len_models < save_n:
            raise ValueError('Not enough models: {}'.format(len_models))
        for n_model in range(save_n):

            model = self.MODELS[n_model]
            model = model[self.model_tag]
            model_json_string = model.to_json()
            if n_model == 0:
                n_model = 'best'
                path = '{}\\{}_{}'.format(save_dir, self.model_tag, 'best')
            if not os.path.exists(path):
                os.mkdir(path)
            with open('{}\\architecture.json'.format(path), 'w') as out:
                json.dump(model_json_string, out)

            model.save_weights(
                '{}\\weights.h5'.format(path)
            )

        return None

    def load_m(self, model_name,  loss, optimizer, metrics=None,
               load_dir='results\\saved_models'):
        """Method for loading a keras model, the model will become best_model
        Args:
            - model_name: string, name of the model we want to load
            - load_dir: string, directory where the keras model is located
        Returns:
            - loaded_model: keras model, the model we loaded
        """
        path = '{}\\{}'.format(load_dir, model_name)
        with open('{}\\architecture.json'.format(path)) as inp:
            loaded_model = json.load(inp)

        loaded_model = model_from_json(loaded_model)
        loaded_model.compile(
            loss=loss,
            optimizer=optimizer,
            metrics=metrics
        )
        loaded_model.load_weights(
            '{}\\weights.h5'.format(path)
        )
        setattr(self, 'best_model', loaded_model)
        return loaded_model

    def fit(self, X, y, **kwargs):
        """Method warpping keras fit method.
        For avoiding problems when doing cross-validation we clone the model
        and re-set the weights for the model.
        Args:
            - X: numpy array, input data for fitting the model
            - y: numpy array, output data for fitting the model
            - **kwargs: keyword arguments passed to the model's predict method
        Returns:
            - fitted_model: compiled keras model, cloned version of best_model
                            with its parameters updated with respect to X and y
        """
        if not hasattr(self, 'best_model'):
            raise(AttributeError('No best model is present'))

        self.__clone_best_model()
        self.fitted_model.fit(X, y, **kwargs)
        return self.fitted_model

    def predict(self, X, **kwargs):
        """Method warpping sklearn predict method.
        It uses fitted_model to perform an estimate given X

        Args:
            - X: numpy array, input data for making predictions with the model
            - y: numpy array, ground truth against which the model is tested
            - **kwargs: keyword arguments passed to the model's predict method
        Returns:
            - predictions: numpy array of shape(y.shape[0], y.shape[1]),
                           predictions produced by the model given X.
            - score: float, the outcome of metric(ground_truth, predictions)
        """
        if not hasattr(self, 'fitted_model'):
            raise(AttributeError('No fitted model is present'))

        predictions = self.fitted_model.predict(X, **kwargs)
        return predictions


class AbstractHyperSklearn:
    """ Abstract class implementing methods inherithed by an Hypermodel. An
    Hypermodel is a keras model written to be optimized with kerastuner.
    Since kerastuner uses a different process for optimizing keras and sklearn
    models we can't merge this class with the AbstractHyperNeuralNet.
    """
    MODELS = {}

    def tune(self, X, y, oracle, num_models=5, **kwargs):
        """Method used for tuning the hypermodel which inherit from the
        abstract class.
        Differently from the tuninng of a keras model, sklearn models requires
        to be tuned by the same sk_tuner. Different tuning algorithms are
        passed in the form of a kerastuner oracle.
        Args:
            - X: numpy array, input to the hypermodel
            - y: numpy array, target of the hypermodel
            - oracle: a kerastuner oracle, one of random search, hyperband
                     or gaussian process
            - num_models: integer, number of best models to keep after the
                          tuning
            - **kwargs: keyword aruments that need to be passed to the tuner
        """
        tuner_obj = sk_tuner(
            oracle=oracle,
            hypermodel=self.build,
            **kwargs
        )

        tuner_obj.search(X, y)
        # print the result for each configuration
        tuner_obj.results_summary()
        # we set only the best model as best_model
        best_models = tuner_obj.get_best_models(num_models=num_models)
        setattr(self, 'best_model', best_models[0])
        for index, model in enumerate(best_models):

            model_name = type(model).__name__
            self.MODELS[index] = {
                '{}_{}'.format(model_name, self.model_tag): model
            }

        return self.MODELS

    def save_m(self, save_n=1, save_dir='results\\saved_models'):
        """Method for saving the best n_models
        Args:
            - save_n: integer specifying the number of models to be saved
            - save_dir: string, directory where to save the models, the
                        default value assumes that this method is called
                        level of the hierarchy
        Returns:
            - None
        """
        len_models = len(self.MODELS)
        if len_models < save_n:
            raise ValueError('Not enough models: {}'.format(len_models))
        for model_n in range(save_n):

            model = self.MODELS[model_n]
            model_name = list(model.keys())[0]
            model = list(model.values())[0]
            if model_n == 0:
                model_n = 'best'
            dump(
                model,
                '{}\\{}_{}.joblib'.format(
                    save_dir,
                    model_name,
                    model_n
                )
            )

        return None

    def load_m(self, model_name, load_dir='results\\saved_models'):
        """Method for loading a sklearn model, the model will become best_model
        Args:
            - model_name: string, name of the model we want to load, only
                          joblib format is accepted
            - load_dir: string, directory where the keras model is located
        Returns:
            - loaded_model: keras model, the model we loaded
        """
        loaded_model = load(
            '{}\\{}.joblib'.format(load_dir, model_name)
        )
        setattr(self, 'best_model', loaded_model)
        return loaded_model

    def fit(self, X, y, **kwargs):
        """Method warpping sklearn fit method.
        For avoiding problems when doing cross-validation we do a deep-copy of
        best model, set it as fitted_model and fit on the data with that model.
        Args:
            - X: numpy array, input data for fitting the model
            - y: numpy array, output data for fitting the model
            - **kwargs: keyword arguments passed to the model's predict method
        Returns:
            - fitted_model: compiled sklearn model, cloned version of
                            best_model with its parameters updated with respect
                            to X and y
        """
        if not hasattr(self, 'best_model'):
            raise(AttributeError('No best model is present'))

        setattr(self, 'fitted_model', clone(self.best_model))
        self.fitted_model.fit(X, y, **kwargs)
        return self.fitted_model

    def predict(self, X, **kwargs):
        """Method warpping sklearn predict method.
        It uses fitted_modelto perform an estimate given X
        Args:
            - X: numpy array, input data for making predictions with the model
            - y: numpy array, ground truth against which the model is tested
            - **kwargs: keyword arguments passed to the model's predict method
        Returns:
            - predictions: numpy array of shape(y.shape[0], y.shape[1]),
                           predictions produced by the model given X.
        """
        if not hasattr(self, 'fitted_model'):
            raise(AttributeError('No fitted model is present'))

        # fucking sklearn returns the probability over all the classes
        predictions = self.fitted_model.predict_proba(X, **kwargs)[:, 1]
        predictions = predictions.reshape(-1, 1)
        return predictions
